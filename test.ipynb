{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ff11c1",
   "metadata": {},
   "source": [
    "**시나리오와 무관 공통 부분**\n",
    "\n",
    "1. raw_data(.pdf, .hwp파일)을 전체 pdf로 변경\n",
    "2. 청크된 문서 로드  \n",
    "    2-1. chunk.pkl 파일이 없다면? -> pdf data로 부터 생성\n",
    "    2-2. chunk.pkl 파일이 있다면? -> 바로 Load\n",
    "3. 정답 스팬 생성  \n",
    "    3-1. 2.의 청크된 문서로 부터 생성된 정답 스팬이 있다면? -> 생략  \n",
    "    3-1. 2.의 청크된 문서로 부터 생성된 정답 스팬이 없다면? -> 생성  \n",
    "\n",
    "**다음 부터는 시나리오에 따라 나누어짐**\n",
    "\n",
    "4. 임베딩 모델을 선정하여 2.의 청크를 임베딩 및 벡터스토어 생성  \n",
    "    - 임베딩 데이터를 저장해야하는가? -> 사실 faiss_index.idx만 있어도 속도가 나오는데  \n",
    "\n",
    "===[실험 준비 끝]===  \n",
    "\n",
    "5. 질문 입력   \n",
    "    5.1. 준비한 질문이 있다면? -> 직접 질문 입력  \n",
    "    5.2. 준비한 질문이 없다면? -> 공통된 질문을 자동으로 입력\n",
    "6. 질문과 답변, 참고한 문서 등을 기록\n",
    "7. 6.을 바탕으로 평가\n",
    "8. 7.의 평가를 저장\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c7a31",
   "metadata": {},
   "source": [
    "# 기본 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c4ed55",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b812fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import experiment\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b78c3e",
   "metadata": {},
   "source": [
    "## OpenAI API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73eba1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셀 내용 지우시고 각자의 방식으로 API KEY 설정하시면 됩니다.\n",
    "from APIkey_loader import OpenAi_API_KEY\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OpenAi_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230a2e7",
   "metadata": {},
   "source": [
    "## 변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80341508",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"test20250804\"\n",
    "version=\"2.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636d03a",
   "metadata": {},
   "source": [
    "# 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e103418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 실험 구조 설정\n",
      "실험 설정 파일을 불러옵니다.\n",
      "실험 설정을 불러왔습니다.: {'version': '2.0', 'test_name': 'test20250804', 'embedding_model': 'text-embedding-3-small', 'llm_model': 'gpt-4.1-mini', 'chunk_size': 100, 'chunk_overlap': 10, 'top_k': 10}\n",
      "README.md 파일이 존재합니다. 실험 내용을 기록해주세요.\n",
      "\n",
      "[2] Document Chunking\n",
      "청크 파일이 존재합니다.\n",
      "청크 문서를 로드 완료하였습니다.\n",
      "문서 개수: 139494\n",
      "\n",
      "[3] 정답 스팬 생성\n",
      "정답 스팬이 ./data/processed/span//span_list_from_chunksize_100_overlap10.json에 저장되었습니다.\n",
      "정답 스팬을 생성하였습니다.\n",
      "\n",
      "[4] 임베딩 및 리트리버 생성\n",
      "임베딩을 설정하였습니다. \n",
      "모델: text-embedding-3-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Sprint\\Project\\Project_2\\codeit_mid_project\\utils\\experiment.py:266: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings(model=embedding_model_name)\n",
      "c:\\Users\\User\\Sprint\\Project\\Project_2\\codeit_mid_project\\rag_chain\\RagChain_temp.py:25: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  self.llm = ChatOpenAI(model_name=model_name, temperature=0.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS 인덱스를 불러왔습니다\n",
      "retriever를 생성하였습니다. Top K: 10\n",
      "\n",
      "[5] RAG Chain\n",
      "RAG Chain을 생성했습니다. llm 모델: gpt-4.1-mini\n",
      "\n",
      "[6] 질의 테스트를 시작합니다\n",
      "질의 테스트를 종료합니다.\n",
      "응답이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "experiment.experiment(experiment_name=experiment_name, version=version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c724d2",
   "metadata": {},
   "source": [
    "# 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8c7b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = f\"experiment/{experiment_name}/experiment_config.json\"\n",
    "\n",
    "with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    config = json.load(f)\n",
    "embedding_model_name = config.get(\"embedding_model\")\n",
    "llm_model_name = config.get(\"llm_model\")\n",
    "chunk_size = config.get(\"chunk_size\")\n",
    "chunk_overlap = config.get(\"chunk_overlap\")\n",
    "top_k = config.get(\"top_k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe3805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리트리버 평가\n",
    "from utils import evaluate\n",
    "\n",
    "# 파일 경로 지정\n",
    "retrieved_path = f\"experiment/{experiment_name}/result.jsonl\"\n",
    "gold_path = f\"data/processed/span/span_list_from_chunksize_{chunk_size}_overlap{chunk_overlap}.json\"\n",
    "\n",
    "# 데이터 불러오기\n",
    "retrieved_dict = evaluate.load_jsonl(retrieved_path)\n",
    "gold_dict = evaluate.load_gold(gold_path)\n",
    "\n",
    "# 점수 계산 및 출력\n",
    "evaluate.evaluate_recall_at_k(retrieved_dict, gold_dict, k=3)\n",
    "evaluate.evaluate_ndcg_at_k(retrieved_dict, gold_dict, k=3)\n",
    "# evaluate.f1_score_at_k(retrieved_dict, gold_dict, k=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
